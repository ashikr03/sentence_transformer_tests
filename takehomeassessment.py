# -*- coding: utf-8 -*-
"""takehomeassessment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YaX10DEn8hotex2uzlaC-yoKptphAE9Z

**Task 0: Make Relevant Imports**
"""

!pip install transformers datasets sentence_transformers

import kagglehub

# Download latest version
path = kagglehub.dataset_download("kashishparmar02/social-media-sentiments-analysis-dataset")

print("Path to dataset files:", path)

import transformers
import sentence_transformers
import torch
import torch.nn as nn
import numpy as np
import datasets

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
device

"""**Task 1: Sentence Transformer Implementation**

Step 1: Load a standard SentenceTransformer model and generate embeddings
"""

generic_model = sentence_transformers.SentenceTransformer("all-MiniLM-L6-v2")
generic_model.to(device)

# Some Input Sentences to Embed
input_sentences = ["I hate this app.",
                   "I love how it works omg",
                   "it's alright, but i wish it was blue",
                   "i never will download this app.",
                   "i might have something else.",
                   "This is the best thing I've ever worked with."]
# Labels representing sentiments attached to input sentences
input_labels = ["negative negative", "positive positive", "neutral positive", "neutral neutral", "neutral negative", "positive positive"]
generic_embeddings = generic_model.encode(input_sentences)
generic_embeddings.shape

generic_similarities = generic_model.similarity(generic_embeddings, generic_embeddings)
generic_similarities.to(device)
# Direct Representation of Similarities
print(generic_similarities.shape)
generic_similarities

def display_similarities(similarities, input_sentences, input_labels, sentence_indices, top_n):
    """
    Display the top-N most similar sentences (excluding self) for selected input sentences.
    Parameters:
    -----------
    similarities : torch 2D array (n_sentences, n_sentences)
        A square matrix where similarities[i][j] represents the similarity score between
        sentence i and sentence j.

    input_sentences : list[str]
        The list of input sentences corresponding to the rows/columns of the similarity matrix.

    input_labels : list[str]
        Labels or categories associated with each input sentence (same order as input_sentences).

    sentence_indices : list[int]
        Indices of the input sentences for which to display top-N most similar sentences.

    top_n : int
        Number of most sentences to display for each.

    Returns: None
    """
    for i in sentence_indices:
        # Get indices sorted by similarity in descending order
        sorted_indices = np.argsort(np.asarray(similarities[i]))[::-1]

        # Exclude the sentence itself
        top_indices = [idx for idx in sorted_indices if idx != i][:top_n]

        print(f"Top {top_n} similar sentences to \n '{input_sentences[i]}'\n label = {input_labels[i]}:")
        for idx in top_indices:
            print(f"  - '{input_sentences[idx]}', label = {input_labels[idx]} (Similarity: {similarities[i][idx]:.4f})")

display_similarities(generic_similarities, input_sentences, input_labels, [0,1,2], 4)

"""Step 2: Custom SentenceTransformer Model : pre-trained HuggingFace Model + weighted pooling"""

class SentenceTransformerModel_custom(nn.Module):
    """
        Custom model for generating sentence embeddings using a pretrained transformer.

        Architecture:
        -------------
        - Encoder: Pretrained transformer model (e.g., BERT) from Hugging Face.
        - Attention-Weighted Pooling: Softmax-weighted sum over token embeddings (excluding [CLS]).
        - Projection: Linear layer to reduce to output_dim.
        - Normalization: L2-normalizes the output embeddings.

        Parameters:
        -----------
        model_name : str
            Pretrained transformer model name or path.
        output_dim : int, optional (default=384)
            Output embedding dimension.

        Inputs:
        -------
        input_ids : torch.Tensor
            Tokenized input IDs (batch_size, seq_len).
        attention_mask : torch.Tensor
            Attention mask (batch_size, seq_len).

        Returns:
        --------
        torch.Tensor
            L2-normalized sentence embeddings (batch_size, output_dim).
    """

    def __init__(self, model_name, output_dim=384):
        super(SentenceTransformerModel_custom, self).__init__()
        self.model = transformers.AutoModel.from_pretrained(model_name)
        hidden_size = self.model.config.hidden_size
        self.pooling = nn.Linear(hidden_size, output_dim)
        self.normalize = nn.functional.normalize
        self.weights = nn.Parameter(torch.randn(hidden_size))

    def forward(self, input_ids, attention_mask):

        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)
        token_embeddings = outputs.last_hidden_state[:, 1:, :]
        softmaxed_embeddings = torch.nn.functional.softmax(token_embeddings, dim=1)
        weighted_sum = torch.sum(token_embeddings * softmaxed_embeddings, dim=1)
        pooled_embeddings = self.pooling(weighted_sum)
        normalized_embeddings = nn.functional.normalize(pooled_embeddings, p=2, dim=1)
        return normalized_embeddings

custom_model = SentenceTransformerModel_custom('nlptown/bert-base-multilingual-uncased-sentiment')
custom_model.to(device)
tokenizer = transformers.AutoTokenizer.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')

#tokenizing the sample sentences such that they can be processed by the transformer
inputs = tokenizer(input_sentences, padding=True, truncation=True, return_tensors="pt")
inputs = inputs.to(device)
input_ids = inputs['input_ids']
attention_mask = inputs['attention_mask']

#encoding
with torch.no_grad():
    custom_embeddings = custom_model(input_ids, attention_mask)
    print(custom_embeddings)
    print(custom_embeddings.shape)

"""Step 3 : Comparing Similarity Scores generated by Generic and Custom Models"""

from sklearn.metrics.pairwise import cosine_similarity

print(generic_embeddings.shape)
print(custom_embeddings.shape)

if generic_embeddings.device != 'cpu':
    generic_embeddings = generic_embeddings.cpu().numpy()

if custom_embeddings.device != 'cpu':
    custom_embeddings = custom_embeddings.cpu().numpy()

# Compute cosine similarity, comparing the "basic" model to the custom model
generic_similarity = cosine_similarity([generic_embeddings[1]], [generic_embeddings[5]])
custom_similarity = cosine_similarity([custom_embeddings[1]], [custom_embeddings[5]])

print(f"Generic Model Similarity for first sentence: {generic_similarity[0][0]}")
print(f"Custom Model Similarity for first sentence: {custom_similarity[0][0]}")

"""Explanation: While I've kept much of the transformer architecture the same, I decided that I would add a custom pooling method outside the transformer backbone such that more semantic information could be captured. For this task, I used a pre-trained model from HuggingFace, and kept the backbone the same. However, as I was attempting to capture sentiment within the embeddings, and I wanted the model to be sensitive towards these aspects, I implemented a normalized, attention-based (with the softmax), weighted sum that generated fixed-length embeddings.

**Task 2: Multi-task Learning Expansion**

I have implemented both Task A and Task B in one ```MultiTaskLearning_ClassificationandSentiment(nn.Module)``` class, so that the model is modular and can be used for both tasks.
"""

class MultiTaskLearning_ClassificationandSentiment(nn.Module):
    """
    MTL model for joint classification and sentiment analysis.

    This model uses a shared SentenceTransformer encoder backbone to extract sentence-level
    embeddings, with two task-specific heads:

    - A classification head for predicting categorical platform types
    - A sentiment analysis head for predicting sentiment categories

    Architecture:
    -------------
    - Shared Encoder: A pre-trained SentenceTransformer model ('nlptown/bert-base-multilingual-uncased-sentiment')
                     used to compute fixed-size embeddings.
    - Classification Head: A two-layer feedforward neural network with ReLU activation.
    - Sentiment Head: A two-layer feedforward neural network with ReLU activation.

    Loss Computation:
    -----------------
    - Computes task-specific cross-entropy loss for both classification and sentiment.
    - If only one task is active (i.e., its labels are provided), only that loss is used.
    - If both tasks are active, the final loss is the average of the two.

    Parameters:
    -----------
    a_class_num : int
        Number of classes for the classification task.
    b_class_num : int
        Number of classes for the sentiment task.

    Inputs:
    -------
    input_ids : torch.Tensor
        Tokenized input IDs for the transformer encoder.
    attention_mask : torch.Tensor, optional
        Attention mask for the encoder input.
    sentiment_labels : torch.Tensor, optional
        Ground-truth sentiment labels for computing sentiment task loss.
    classification_labels : torch.Tensor, optional
        Ground-truth classification labels for computing classification task loss.

    Returns:
    --------
    dict with keys:
        'classification' : torch.Tensor
            Logits for the classification task.
        'sentiment' : torch.Tensor
            Logits for the sentiment task.
        'loss' : torch.Tensor
            Combined or individual task loss, depending on available labels.
    """
    def __init__(self, a_class_num, b_class_num):
        super(MultiTaskLearning_ClassificationandSentiment, self).__init__()
        # shared base encoder for both tasks
        self.shared_encoder = SentenceTransformerModel_custom('nlptown/bert-base-multilingual-uncased-sentiment')
        # output dim of the shared encoder
        hidden_size = self.shared_encoder.pooling.out_features

        # head for the classification task
        self.classification_task = nn.Sequential(nn.Linear(hidden_size, hidden_size // 2),
            nn.ReLU(),
            nn.Linear(hidden_size // 2, a_class_num))

        # head for the sentiment task
        self.sentiment_task = nn.Sequential(nn.Linear(hidden_size, hidden_size // 2),
            nn.ReLU(),
            nn.Linear(hidden_size // 2, b_class_num))

    def forward(self, input_ids, attention_mask = None, sentiment_labels = None, classification_labels = None):
        shared_embeddings = self.shared_encoder(input_ids, attention_mask)

        classification_logits = self.classification_task(shared_embeddings)
        sentiment_logits = self.sentiment_task(shared_embeddings)

        loss = 0
        num_losses = 0

        # If either of the task specific heads are frozen (so the task_labels are None)
        # then train normally for only one of them -> adjust loss fn accordingly.

        if classification_labels is not None:
            classification_loss = nn.CrossEntropyLoss()(classification_logits, classification_labels)
            loss = loss + classification_loss
            num_losses += 1

        if sentiment_labels is not None:
            sentiment_loss = nn.CrossEntropyLoss()(sentiment_logits, sentiment_labels)
            loss = loss + sentiment_loss
            num_losses += 1

        # LOSS = 1/2(class_loss + sent_loss), adjusted appropriately if either is None
        # for task-specific training.
        if num_losses > 0:
            loss = loss / num_losses


        outputs = {
        'classification': classification_logits,
        'sentiment': sentiment_logits,
        'loss': loss}

        return outputs

"""Explanation: In order to modify the the model such that it can handle multi-task learning, I created another class ```MultiTaskLearning_ClassificationandSentiment(nn.Module)``` of Transformer model that took the original model as the "shared encoder" between the two tasks. Then, I added both a classification-specific head and a sentiment analysis-specific head so that the model could appropriately perform its downstream tasks.

I will train/fine-tune this model in Task 4 below.

**Task 3: Training Considerations**

The implications, advantages, and rationale for training the model based on the following criteria as it follows:

1. If the entire network is frozen, that means that no training can occur, and the network will default to the pre-trained model. This means that no learning can take place. Some advantages of this can include: fast deployment and preservation of pre-training -- not having to train the model frees up resources and would work well if the task at hand is similar to the pre-training task. This configuration, however, is not ideal for multi-task learning, or any "new" tasks.
2. If only the transformer backbone is frozen, then not only can the model take advantage of its pre-training, but one can also add task-specific heads that can fine-tune its understanding for new tasks. This configuration is preferable as it balances its pre-trained knowledge and its ability to learn. It is used in transfer learning, where a pre-trained model is leveraged for new tasks.
3. If only one task-specific head is frozen, then this leads to an unbalanced learning, where one head can learn from the data, while the other does not learn. Hence, the model becomes very task-specific. This is ideal for when  there is asymmetrical pre-training, where one task is not represented as well, and thus needs to be focused on in fine-tuning.

Transfer learning can be beneficial where we want to leverage the language understanding of a pre-trained model, and then fine-tune on a specific task. For example, one could use a pre-trained NLP model and fine-tune it to perform sentiment analysis on product reviews.

1. The choice of pre-trained model would be something trained on an immense amount of language data, so either BERT or RoBERTa. That way, we can leverage its robust language capabilites to understand the product reviews, but also fine-tune it such that we can extract the sentiment.
2. In this case, we would need to freeze the transformer backbone and un-freeze the classification head. This is so that we can allow the model adapt to the specific task and "learn" patterns within the task. This way, it is efficient in its resource while also "learning" how to do the new task of extracting sentiment from product reviews.
3. As stated above, freezing the backbone would allow for the preservation of pre-trained knowledge and the ability to learn new tasks. This way, we preserve computational resources but also get task-specific adaptation.

**Task 4: Training Loop Implementation**
"""

import pandas as pd

#importing data to fine-tune the model
df = pd.read_csv("/kaggle/input/social-media-sentiments-analysis-dataset/sentimentdataset.csv")
df.drop(columns=['Unnamed: 0', 'Unnamed: 0.1'], inplace=True)
df["Platform"] = df["Platform"].str.strip()
df["Sentiment"] = df["Sentiment"].str.strip()
df.head()

from sklearn.model_selection import train_test_split
from transformers import AutoTokenizer, TrainingArguments, Trainer, DataCollatorWithPadding
from datasets import Dataset, ClassLabel
import numpy as np

def label_list_to_tensor(label_list : list[str]):
    """
    Converts a list of string labels into a numerical tensor suitable for our PyTorch model.
    The function determines the unique label names, assigns an integer index to each unique label,
    and converts the entire input list into a tensor of indices.

    Parameters:
    -----------
    label_list : list of str
        A list of string labels (e.g., ['Facebook', 'Instagram', 'Twitter', ...]) to be converted.

    Returns:
    --------
    torch.Tensor
        A tensor of shape (len(label_list),) containing integer valued labels.
    """
    # get unique label names -> sort for consistency across runs (hashing is randomized)
    unique_labels = list(sorted(set(label_list))) # Get unique labels
    # enumerate label names -> these are the mappings
    label_to_index = {label: index for index, label in enumerate(unique_labels)}
    indexed_labels = [label_to_index[label] for label in label_list]
    # convert to torch.tensor for model
    label_tensor = torch.tensor(indexed_labels)

    return label_tensor

def preprocess_data(df):
    """
    Preprocesses a DataFrame containing text reviews and their corresponding sentiment and platform labels
    for multi-task learning.
    Parameters:
    -----------
    df : pandas.DataFrame containing three required columns:
        - 'Text': review text (strings)
        - 'Sentiment': sentiment labels (strings, e.g., 'positive', 'neutral', 'negative')
        - 'Platform': classification labels (strings, e.g., 'web', 'ios', 'android')
    Returns:
    --------
    dict
        A dictionary containing:
        - tokenized inputs (e.g., 'input_ids', 'attention_mask', etc.)
        - 'sentiment_labels': torch.Tensor of numerical sentiment class indices
        - 'classification_labels': torch.Tensor of numerical platform class indices
    Notes:
    ------
    - The tokenizer used must be initialized globally before calling this function.
    """
    reviews = df['Text']
    # convert labels to numerical tensor
    sentiments = label_list_to_tensor(df['Sentiment'])
    classifications = label_list_to_tensor(df["Platform"])
    # encode input sentences/reviews as embeddings
    encoding =tokenizer(
              reviews,
              padding='max_length',
              max_length=128,
              truncation=True,
              return_tensors=None)
    # add labels to embeddings for the model
    encoding['sentiment_labels'] = sentiments
    encoding['classification_labels'] = classifications
    return encoding

# Globally initialize tokenizer
tokenizer = AutoTokenizer.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')

# Split and Preprocess train/val data
full_dataset = Dataset.from_pandas(df)
train_dataset, val_dataset = full_dataset.train_test_split(test_size=0.2, shuffle=True, seed=42).values()
train_dataset = train_dataset.map(preprocess_data, batched=True)
val_dataset = val_dataset.map(preprocess_data, batched=True)

# Compute number of sentiment and classification classes for initializing the model
train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask',
                                                'sentiment_labels', 'classification_labels'])
val_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask',
                                              'sentiment_labels', 'classification_labels'])

num_classification_classes = np.unique(df['Platform']).shape[0]
num_sentiment_classes = np.unique(df['Sentiment']).shape[0]

# Initialize Model
model = MultiTaskLearning_ClassificationandSentiment(num_classification_classes, num_sentiment_classes)
model.to(device)

from sklearn.metrics import accuracy_score, f1_score

import os
os.environ["WANDB_DISABLED"] = "true"

training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=10,
    per_device_train_batch_size=16,
    learning_rate=5e-4,
    weight_decay=0.01,
    optim='adamw_torch',
    lr_scheduler_type='linear',
    warmup_steps=500,
    eval_strategy='epoch',
    save_strategy='epoch',
    save_steps=1000,
    load_best_model_at_end=True
)

def compute_metrics(eval_pred):
    # Unpack predictions and labels
    logits, labels = eval_pred

    classification_logits, sentiment_logits = logits
    classification_labels, sentiment_labels = labels

    # For multi-task, logits and labels will be dictionaries
    classification_preds = np.argmax(classification_logits, axis = 1)
    sentiment_preds = np.argmax(sentiment_logits, axis = 1)
    print(classification_preds, sentiment_preds)
    # Calculate metrics for both tasks
    classification_acc = accuracy_score(classification_labels, classification_preds)
    classification_f1 = f1_score(classification_labels, classification_preds, average='weighted')

    sentiment_acc = accuracy_score(sentiment_labels, sentiment_preds)
    sentiment_f1 = f1_score(sentiment_labels, sentiment_preds, average='weighted')

    # Calculate average metrics across tasks
    average_acc = (classification_acc + sentiment_acc) / 2
    average_f1 = (classification_f1 + sentiment_f1) / 2

    return {
        'classification_acc': classification_acc,
        'classification_f1': classification_f1,
        'sentiment_acc': sentiment_acc,
        'sentiment_f1': sentiment_f1,
        'average_acc': average_acc,
        'average_f1': average_f1
    }

from torch.utils.data import default_collate

def custom_collate(batch):
    return default_collate(batch)

# Initialize the trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    compute_metrics=compute_metrics,
    data_collator=custom_collate
    )

trainer.train()

from datasets import load_dataset

ds = load_dataset("stanfordnlp/imdb")
ds_train = ds["train"]
ds_test = ds["test"]
ds_train = ds_train.rename_column('label', 'sentiment_labels')
ds_test = ds_test.rename_column('label', 'sentiment_labels')
ds_train = ds_train.add_column('classification_labels', np.zeros(len(ds_train), dtype=np.int64))
ds_test = ds_test.add_column('classification_labels', np.zeros(len(ds_test), dtype=np.int64))


tokenizer = AutoTokenizer.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')
def tokenization(batch):
    return tokenizer(batch['text'], padding=True, truncation=True)

ds_train = ds_train.map(tokenization, batched=True, batch_size=None)
ds_test = ds_test.map(tokenization, batched=True, batch_size=None)
ds_train.set_format('torch', columns=['input_ids', 'attention_mask', 'sentiment_labels', 'classification_labels'])
ds_test.set_format('torch', columns=['input_ids', 'attention_mask', 'sentiment_labels', 'classification_labels'])

sent_nums = 2

model = MultiTaskLearning_ClassificationandSentiment(2, sent_nums)
model.to(device)

training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=3,
    per_device_train_batch_size=16,
    learning_rate=5e-4,
    weight_decay=0.01,
    optim='adamw_torch',
    lr_scheduler_type='linear',
    warmup_steps=500,
    eval_strategy='epoch',
    save_strategy='epoch',
    save_steps=1000,
    load_best_model_at_end=True
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=ds_train,
    eval_dataset=ds_test,
    compute_metrics=compute_metrics,
    data_collator=custom_collate
    )


trainer.train()

!pip freeze > requirements.txt

from google.colab import files
files.download('requirements.txt')